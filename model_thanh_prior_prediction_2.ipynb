{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPL Dataset\\2005-2006.csv\n",
      "EPL Dataset\\2006-2007.csv\n",
      "EPL Dataset\\2007-2008.csv\n",
      "EPL Dataset\\2008-2009.csv\n",
      "EPL Dataset\\2009-2010.csv\n",
      "EPL Dataset\\2010-2011.csv\n",
      "EPL Dataset\\2011-2012.csv\n",
      "EPL Dataset\\2012-2013.csv\n",
      "EPL Dataset\\2013-2014.csv\n",
      "EPL Dataset\\2014-2015.csv\n",
      "EPL Dataset\\2015-2016.csv\n",
      "EPL Dataset\\2016-2017.csv\n",
      "EPL Dataset\\2017-2018.csv\n",
      "EPL Dataset\\2018-2019.csv\n",
      "EPL Dataset\\2019-2020.csv\n",
      "EPL Dataset\\2020-2021.csv\n",
      "EPL Dataset\\2021-2022.csv\n",
      "EPL Dataset\\2022-2023.csv\n",
      "EPL Dataset\\2023-2024.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a joined pandas file\n",
    "joined_files = os.path.join(\"EPL Dataset\", \"*.csv\")\n",
    "joined_list = glob.glob(joined_files)\n",
    "li = []\n",
    "\n",
    "for filename in joined_list[12:]:\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "dataset = pd.concat(li, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('EPL_data_for_2.5_prediction_no_bet.csv', index_col=None, header=0)\n",
    "df_features = dataset.iloc[:, 11:]\n",
    "df_features = df_features.interpolate(method='linear')\n",
    "result = dataset['FTR']\n",
    "\n",
    "result = pd.factorize(result)\n",
    "features = np.array(df_features)\n",
    "\n",
    "scaler = Normalizer()\n",
    "\n",
    "features = scaler.fit_transform(features)\n",
    "result = np.array(result[0])\n",
    "train_size = int(len(features) * 0.8)\n",
    "X_train, X_val = features[:train_size], features[train_size:]\n",
    "y_train, y_val = result[:train_size], result[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 ... 2 1 2]\n",
      "0.5198581560283688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = SGDClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "print(y_pred)\n",
    "print(accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5347517730496454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "print(accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2.14, 2.33, 1.72, 1.19, 0, 1.46, 0, 0, 1.38, 0, 2.22, 1.46, 1.8, 0, 1.16, 0, 0, 1.62, 0, 0, 1.38, 2.07, 1.88, 0, 0, 1.35, 2.16, 2.35, 0, 0, 0, 1.14, 0, 1.18, 1.85, 4.1, 0, 2.1, 1.38, 0, 2.5, 0, 0, 2.41, 0, 0, 1.54, 1.37, 3.4, 0, 2.13, 2.05, 0, 1.9, 0, 3.15, 0, 1.1, 1.45, 2.25, 0, 0, 0, 1.35, 0, 2.04, 0, 1.71, 2.52, 1.59, 1.7, 0, 1.57, 0, 0, 2.26, 1.69, 2.81, 1.59, 0, 0, 0, 0, 0, 0, 0, 3.87, 2.62, 1.54, 1.47, 1.81, 0, 0, 0, 1.82, 1.6, 1.56, 4.15, 0, 0, 0, 1.53, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.29, 2.88, 0, 0, 0, 1.68, 2.1, 1.35, 1.59, 1.43, 0, 0, 0, 1.67, 2.56, 0, 0, 2.1, 2.51, 0, 1.44, 1.88, 1.56, 0, 0, 0, 1.54, 0, 4.35, 1.38, 0, 2.53, 0, 1.99, 0, 0, 0, 0, 1.19, 3.77, 2.72, 0, 0, 3.81, 0, 2.38, 0, 1.15, 2.12, 1.58, 0, 2.08, 2.1, 1.57, 0, 0, 3.32, 2.63, 0, 0, 1.76, 0, 0, 0, 2.11, 5.8, 0, 0, 1.83, 0, 0, 1.85, 0, 0, 1.5, 0, 1.53, 2.53, 0, 0, 3.79, 1.77, 2.06, 0, 1.55, 0, 2.01, 0, 4.45, 1.15, 1.95, 1.82, 0, 0, 0, 0, 0, 0, 2.48, 0, 2.12, 1.6, 0, 0, 1.81, 0, 0, 0, 1.66, 1.72, 0, 0, 2.58, 1.52, 0, 1.21, 0, 0, 0, 0, 1.91, 0, 1.85, 0, 0, 1.19, 1.47, 1.62, 3.04, 1.29, 0, 0, 1.55, 2.5, 2.0, 2.18, 1.23, 4.03, 0, 0, 0, 0, 2.3, 0, 0, 1.16, 0, 0, 0, 1.38, 0, 0, 3.2, 2.14, 0, 1.49, 0, 1.24, 2.04, 0, 3.48, 0, 2.25, 2.87, 0, 3.15, 0, 0, 1.5, 0, 2.2, 1.52, 1.93, 3.6, 0, 1.37, 0, 0, 0, 1.96, 4.24, 1.79, 1.29, 0, 1.38, 2.46, 0, 0, 0, 2.27, 2.78, 2.5, 1.57, 1.32, 0, 2.42, 1.26, 0, 0, 0, 0, 0, 1.6, 0, 0, 0, 1.25, 0, 3.9, 0, 0, 0, 0, 0, 2.18, 0, 0, 0, 0, 0, 1.53, 1.62, 2.18, 1.23, 0, 0, 0, 0, 1.38, 2.7, 1.6, 0, 1.92, 1.82, 0, 1.72, 0, 0, 0, 1.56, 1.63, 0, 2.17, 0, 2.33, 1.7, 0, 2.67, 0, 0, 1.56, 1.57, 0, 0, 0, 1.63, 3.7, 0, 0, 3.97, 1.74, 0, 1.4, 0, 0, 1.86, 1.42, 1.63, 0, 0, 1.86, 0, 0, 0, 0, 1.5, 0, 1.38, 2.24, 1.53, 0, 2.13, 1.33, 0, 0, 0, 0, 0, 0, 1.32, 3.5, 1.87, 0, 1.54, 0, 0, 2.15, 5.3, 0, 0, 1.34, 0, 2.08, 0, 3.16, 1.58, 1.27, 0, 0, 0, 0, 0, 1.96, 1.79, 0, 0, 1.28, 1.67, 1.67, 7.0, 0, 1.49, 0, 1.19, 1.48, 0, 1.76, 0, 0, 0, 0, 1.3, 2.04, 1.75, 3.3, 1.37, 0, 5.85, 1.24, 0, 0, 0, 1.13, 1.8, 0, 2.55, 1.83, 2.66, 1.3, 0, 0, 0, 1.97, 0, 0, 0, 1.48, 0, 0, 1.55, 0, 1.63, 1.24, 0, 2.38, 1.33, 1.83, 1.8, 0, 0, 0, 1.25, 0, 3.6, 2.5, 2.88, 0, 0, 2.9, 0, 1.68, 2.4, 0, 0, 0, 0, 2.38, 0, 0, 0, 1.42, 1.83, 1.75, 0, 0, 0, 0, 0, 1.39, 0, 0, 1.14, 0, 2.48, 1.66, 0, 0, 0, 2.09, 1.22, 0, 0, 0, 0, 1.46, 0, 2.48, 0, 0, 0, 0, 0, 1.42, 2.45, 0, 2.17, 2.57, 2.32, 2.08, 1.76, 0, 0, 2.62, 0, 1.43, 0, 0, 5.0, 1.84, 2.52, 0, 0, 0, 6.7, 0, 1.53, 1.22, 1.8, 1.52, 0, 1.33, 0, 0, 2.34, 0, 1.34, 0, 0, 2.52, 0, 1.35, 0, 0, 1.41, 1.44, 1.7, 0, 0, 2.44, 0, 1.47, 1.23, 0, 0, 1.45, 2.51, 0, 0, 1.19, 1.71, 1.32, 1.27, 0, 0, 1.78, 2.36, 0, 1.18, 0, 0, 2.28, 0, 1.15, 1.98, 1.21, 0, 0, 1.22, 1.46, 1.76, 0, 1.75, 2.02, 0, 1.74, 0, 0, 0, 0, 1.19, 1.42, 0, 1.66, 0, 3.12, 2.9, 2.15, 0, 0, 2.18, 1.34, 0, 1.77, 0, 0, 2.4, 0, 1.33, 0, 0, 2.47, 1.88, 0, 2.76, 2.63, 0, 1.97, 0, 0, 1.63, 0, 0, 0, 2.66, 1.63, 0, 1.14, 0, 0, 0, 1.29, 2.28, 0, 0, 0, 0, 1.2, 1.32, 2.8, 0, 0, 1.85, 0, 1.5, 0, 0, 1.7, 1.14, 2.15, 0, 1.95, 2.36, 5.0, 0, 1.18, 1.7, 1.63, 1.94, 3.16, 0, 0, 0, 1.32, 2.2, 0, 1.86, 0, 1.54, 2.88, 2.68, 3.52, 1.33, 1.65, 1.45, 1.66, 1.32, 0, 1.88, 2.64, 1.58, 2.3, 2.24, 1.36, 0, 1.67, 0, 0, 1.58, 0, 2.38, 0, 0, 0, 0, 2.26, 1.8, 1.16, 0, 1.26, 0, 0, 3.13, 0, 1.85, 1.56, 4.25, 0, 0, 4.74, 0, 0, 2.69, 0, 2.86, 0, 3.15, 0, 0, 1.28, 4.05, 2.65, 2.36, 0, 1.51, 0, 0, 2.44, 1.23, 2.95, 2.18, 0, 1.14, 2.26, 0, 0, 2.91, 1.59, 1.22, 0, 0, 0, 1.43, 3.14, 2.84, 0, 1.33, 0, 1.49, 1.84, 0, 2.28, 0, 0, 1.74, 2.86, 0, 1.5, 0, 0, 1.2, 1.56, 1.7, 1.49, 0, 1.29, 0, 1.38, 0, 0, 2.1, 0, 0, 0, 0, 1.42, 0, 0, 0, 1.4, 0, 2.5, 0, 1.22, 3.23, 1.95, 1.19, 1.2, 1.29, 0, 0, 4.0, 2.42, 0, 1.4, 1.67, 0, 6.12, 1.41, 1.85, 1.6, 0, 1.09, 0, 0, 4.04, 4.4, 0, 0, 1.44, 2.6, 0, 0, 0, 1.47, 0, 3.29, 0, 0, 0, 0, 1.96, 1.46, 1.14, 1.22, 1.36, 0, 0, 1.51, 0, 3.44, 5.5, 0, 1.5, 0, 1.1, 0, 1.34, 0, 0, 2.25, 1.59, 0, 0, 1.53, 2.32, 0, 1.92, 0, 0, 2.68, 1.37, 0, 1.54, 1.86, 3.05, 2.15, 0, 0, 0, 0, 0, 2.02, 1.4, 0, 1.8, 3.65, 1.44, 1.14, 1.88, 0, 2.17, 1.84, 2.69, 2.15, 0, 0, 0, 0, 1.78, 1.52, 0, 1.6, 0, 0, 0, 0, 2.34, 0, 0, 1.48, 1.69, 0, 0, 2.4, 0, 0, 1.23, 0, 2.23, 0, 0, 3.46, 3.72, 1.59, 1.33, 0, 0, 3.17, 2.0, 1.88, 0, 0, 1.24, 1.77, 1.77, 1.16, 0, 2.34, 3.15, 2.85, 0, 1.96, 0, 0, 0, 2.94, 1.28, 0, 1.68, 0, 2.42, 1.59, 0, 0, 0, 0, 0, 2.28, 2.22, 1.8, 1.55, 1.36, 1.35, 1.33, 0, 1.33, 1.98, 0, 2.09, 0, 0, 0, 0, 0, 0, 0, 2.18, 3.3, 1.32, 0, 0, 0, 0, 1.74, 3.52, 2.0, 4.36, 0, 3.4, 2.8, 2.8, 1.71, 1.7, 1.56, 0, 0, 1.4, 0, 0, 0, 0, 1.85, 0, 0, 1.22, 1.91, 0, 0, 0, 0, 1.94, 1.38, 1.44, 4.2, 0, 2.71, 0, 0, 0, 0, 0, 0, 3.52, 0, 0, 0, 1.27, 1.58, 0, 1.87, 0, 0, 0, 2.55, 0, 0, 0, 1.66, 1.88, 0, 0, 2.05, 1.74, 1.83, 1.33, 0, 2.65, 1.4, 1.52, 1.52, 1.24, 2.12, 1.84, 1.67, 3.41, 2.89, 0, 2.5, 2.05, 0, 2.8, 0, 2.08, 1.48, 1.38, 1.63, 0, 0, 1.72, 1.49, 0, 1.74, 1.76, 0, 0, 3.56, 0, 1.29, 1.7, 1.31, 0, 0, 2.62, 0, 0, 1.8, 2.49, 0, 1.61, 1.75, 2.85, 0, 1.71, 0, 0, 1.65, 2.15, 0, 0, 0, 0, 1.27, 0, 0, 0, 0, 0, 2.75, 0, 2.59, 1.18, 0, 1.75, 0, 0, 2.47, 0, 0, 0, 1.21, 0, 1.92, 0, 1.84, 0, 0, 0, 1.92, 1.61, 1.81, 0, 0, 2.88, 1.7, 1.54, 2.67, 1.28, 1.83, 1.35, 1.6, 0, 1.64, 1.3, 1.23, 2.08, 0, 1.19, 1.87, 0, 1.52, 0, 0, 2.8, 0, 2.2, 0, 2.38, 0, 1.91, 1.41, 0, 2.1, 1.39, 0, 1.64, 2.07, 0, 1.55, 0, 0, 0, 0, 2.55, 1.25, 1.28, 0, 0, 1.7, 1.43, 2.1, 4.05, 0, 0, 1.53, 2.45, 2.0, 1.56, 0, 1.39, 1.21, 0, 1.36, 0, 2.48, 1.8, 0, 0, 1.34, 2.0, 0, 1.26, 1.97, 2.95, 0, 0, 3.86, 0, 1.33, 2.05, 0, 0, 0, 1.34, 0, 2.22, 1.21, 0, 1.78, 0, 0, 1.93, 0, 1.18, 2.62, 2.01, 1.71, 1.83, 1.51, 1.96, 1.73, 3.07, 1.31, 1.53, 1.58, 0, 1.57, 0, 0, 0, 1.17, 0, 1.8, 0, 1.43, 0, 1.45, 1.51, 2.65, 1.58, 0, 0, 1.36, 1.49, 0, 3.08, 0, 2.1, 0, 1.54, 0, 2.03, 1.68, 1.74, 0, 0, 0, 0, 3.05, 1.4, 0, 1.9, 1.46, 1.5, 0, 0, 1.46, 1.95, 1.55, 1.83, 0, 1.14, 2.19, 0, 0, 1.31, 0, 1.3, 0, 0, 2.12, 0, 0, 1.11, 0, 2.89, 0, 0, 0, 3.41, 1.22, 0, 1.38, 0, 1.63, 0, 1.45, 1.93, 0, 0, 0, 0, 0, 2.25, 0, 1.8, 2.9, 0, 0, 1.3, 1.51, 1.89, 2.67, 2.0, 0, 1.78, 1.29, 0, 0, 0, 1.25, 1.98, 0, 2.14, 1.29, 0, 2.93, 3.3, 4.25, 1.63, 0, 6.0, 0, 0, 0, 3.15, 2.49, 1.17, 2.3, 1.78, 1.28, 0, 0, 0, 1.53, 0, 2.27, 0, 0, 0, 2.72, 0, 0, 2.66, 1.87, 0, 3.91, 0, 2.27, 0, 1.43, 0, 0, 1.75, 0, 2.57, 0, 1.56, 1.4, 2.31, 1.1, 2.75, 3.41, 0, 1.82, 1.51, 0, 0, 1.69, 1.6, 0, 0, 1.31, 1.94, 0, 1.77, 0]\n",
      "1.0474822695035468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trung\\AppData\\Local\\Temp\\ipykernel_21672\\1003983758.py:1: DtypeWarning: Columns (78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv('EPL_2005_2023.csv', index_col=None, header=0)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('EPL_2005_2023.csv', index_col=None, header=0)\n",
    "counter = 0\n",
    "test = []\n",
    "for i in range(0, len(y_val)):\n",
    "    if(y_val[i] == y_pred[i]):\n",
    "        if (y_val[i] == 0):\n",
    "            test.append(dataset.loc[train_size + i]['MaxD'])\n",
    "        if (y_val[i] == 1):\n",
    "            test.append(dataset.loc[train_size + i]['MaxA'])    \n",
    "        if (y_val[i] == 2):\n",
    "            test.append(dataset.loc[train_size + i]['MaxH'])\n",
    "    else:\n",
    "        test.append(0)\n",
    "print(test)\n",
    "print(sum(test) / len(test))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
